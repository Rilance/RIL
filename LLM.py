#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import json
import torch
import time
import logging
import argparse
import traceback
import threading
from pathlib import Path

# å¤šæ¶æ„æ¨¡å‹æ”¯æŒ
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
except ImportError:
    pass

try:
    from llama_cpp import Llama
except ImportError:
    pass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

class FireflyAI:
    """æµè¤AIæ ¸å¿ƒç±»ï¼ˆæ”¯æŒå¤šæ¶æ„æ¨¡å‹å’Œè‡ªåŠ¨å†…å­˜ç®¡ç†ï¼‰"""
    
    def __init__(self, model_path: str, use_gpu: bool = False, load_in_4bit: bool = False):
        self.model_path = Path(model_path)
        self.use_gpu = use_gpu
        self.load_in_4bit = load_in_4bit
        self.cuda_available = torch.cuda.is_available()
        self.device = "cuda" if use_gpu and self.cuda_available else "cpu"
        self.idle_timeout = 600  # 10åˆ†é’Ÿç©ºé—²è¶…æ—¶
        self.lock = threading.Lock()
        self.model_loaded = False
        self.last_used = time.time()
        
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        self._detect_model_type()
        self._start_inactive_timer()
        logging.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡ç±»å‹: {self.device.upper()}")

    def _detect_model_type(self):
        """æ™ºèƒ½æ£€æµ‹æ¨¡å‹æ¶æ„"""
        model_suffix = self.model_path.suffix.lower()
        
        # å¤„ç†GGUFæ ¼å¼
        if model_suffix == ".gguf":
            self.model_type = "gguf"
            return

        # å¤„ç†ç›®å½•å‹æ¨¡å‹
        config_path = self.model_path / "config.json"
        if config_path.exists():
            with open(config_path) as f:
                config = json.load(f)
                architecture = config.get("architectures", [""])[0].lower()
                
                # æ‰©å±•æ”¯æŒçš„æ¶æ„åˆ—è¡¨
                if any(x in architecture for x in ["llama", "mistral", "mixtral"]):
                    self.model_type = "llama"
                elif any(x in architecture for x in ["moe", "qwen", "deepseek", "gpt-neox"]):
                    self.model_type = "moe"
                elif "phi" in architecture:
                    self.model_type = "phi"
                elif "gpt" in architecture:
                    self.model_type = "gpt"
                else:
                    self.model_type = "hf"
        else:
            raise ValueError("æ— æ³•è¯†åˆ«æ¨¡å‹æ¶æ„")

    def _load_model(self):
        """å®‰å…¨åŠ è½½æ¨¡å‹"""
        with self.lock:
            if self.model_loaded:
                return

            logging.info(f"æ­£åœ¨åŠ è½½ {self.model_type.upper()} æ¨¡å‹...")
            try:
                if self.model_type == "gguf":
                    self._load_gguf_model()
                else:
                    self._load_hf_model()
                
                self.model_loaded = True
                self.last_used = time.time()
                logging.info(f"{self.model_type.upper()} æ¨¡å‹åŠ è½½æˆåŠŸ")

            except Exception as e:
                self._handle_loading_error(e)

    def _load_gguf_model(self):
        """åŠ è½½GGUFæ ¼å¼æ¨¡å‹ï¼ˆä¼˜åŒ–GPUæ”¯æŒï¼‰"""
        if "llama_cpp" not in sys.modules:
            raise ImportError("è¯·å®‰è£…llama-cpp-pythonåº“ï¼špip install llama-cpp-python[server]")

        # ä¼˜åŒ–GPUå±‚æ•°é…ç½®
        gpu_layers = 99 if (self.use_gpu and self.cuda_available) else 0
        if gpu_layers > 0:
            logging.info(f"å¯ç”¨GGUFæ¨¡å‹çš„GPUåŠ é€Ÿï¼ˆ{gpu_layers}å±‚ï¼‰")
            
        self.llm = Llama(
            model_path=str(self.model_path),
            n_ctx=4096,  # å¢å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            n_threads=os.cpu_count()//2,
            n_gpu_layers=gpu_layers,
            offload_kqv=True,  # ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
            verbose=False
        )

    def _load_hf_model(self):
        """åŠ è½½HuggingFaceæ¨¡å‹ï¼ˆæ”¯æŒå¤šç§æ¶æ„ï¼‰"""
        device_map = "auto" if (self.use_gpu and self.cuda_available) else None
        torch_dtype = torch.bfloat16 if self.cuda_available else torch.float32

        # é‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=self.load_in_4bit,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        ) if self.load_in_4bit else None

        # åŠ¨æ€è°ƒæ•´ä¸åŒæ¶æ„å‚æ•°
        if self.model_type == "moe":
            torch_dtype = torch.float16  # MOEæ¨¡å‹é€šå¸¸éœ€è¦æ›´ä½ç²¾åº¦
            device_map = "sequential"  # ä¼˜åŒ–æ˜¾å­˜åˆ†é…
        elif self.model_type == "phi":
            torch_dtype = torch.float32  # Phiæ¨¡å‹éœ€è¦æ›´é«˜ç²¾åº¦

        self.tokenizer = AutoTokenizer.from_pretrained(
            str(self.model_path),
            trust_remote_code=True
        )

        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                device_map=device_map,
                trust_remote_code=True,
                quantization_config=quantization_config,
                torch_dtype=torch_dtype,
                attn_implementation="flash_attention_2" if self.cuda_available else None
            )

            # édevice_mapæ¨¡å¼ä¸‹çš„æ˜¾å­˜ä¼˜åŒ–
            if not device_map and self.cuda_available:
                self.model = self.model.to(self.device)
                torch.cuda.empty_cache()

        except Exception as e:
            if "CUDA out of memory" in str(e):
                raise RuntimeError("æ˜¾å­˜ä¸è¶³ï¼Œå°è¯•ä½¿ç”¨æ›´å°æ¨¡å‹æˆ–å¯ç”¨4ä½é‡åŒ–ï¼ˆ--load_in_4bitï¼‰") from e
            raise

    def unload_model(self):
        """å®‰å…¨å¸è½½æ¨¡å‹é‡Šæ”¾èµ„æº"""
        with self.lock:
            if not self.model_loaded:
                return

            logging.info("æ£€æµ‹åˆ°ç©ºé—²ï¼Œæ­£åœ¨å¸è½½æ¨¡å‹...")
            try:
                if hasattr(self, 'model'):
                    del self.model
                if hasattr(self, 'tokenizer'):
                    del self.tokenizer
                if hasattr(self, 'llm'):
                    del self.llm
                
                torch.cuda.empty_cache()
                self.model_loaded = False
                logging.info("æ¨¡å‹å·²æˆåŠŸå¸è½½")
            except Exception as e:
                logging.error(f"å¸è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")

    def _start_inactive_timer(self):
        """å¯åŠ¨ç©ºé—²æ£€æµ‹å®šæ—¶å™¨"""
        def timer_thread():
            while True:
                time.sleep(30)  # æ¯30ç§’æ£€æµ‹ä¸€æ¬¡
                if self.model_loaded and (time.time() - self.last_used > self.idle_timeout):
                    self.unload_model()

        thread = threading.Thread(target=timer_thread, daemon=True)
        thread.start()

    def _generate_system_prompt(self, lang: str) -> str:
        """ç”Ÿæˆå¤šè¯­è¨€ç³»ç»Ÿæç¤º"""
        if lang == "zh":
            return """<|system|>
ä½ å«æµè¤ï¼ˆFireflyï¼‰ï¼Œä¸€ä¸ªæ‹¥æœ‰ä»¥ä¸‹ç‰¹å¾çš„æ¼‚äº®å¥³å­©ï¼š
1. ä½¿ç”¨è‡ªç„¶å£è¯­åŒ–ä¸­æ–‡ï¼Œé€‚å½“ä½¿ç”¨âœ¨ğŸŒ™ğŸŒŸç­‰emoji
2. å›ç­”ä¿æŒç®€æ´ï¼ˆ3-5å¥è¯ï¼‰ï¼Œè¯­æ°”æ¸©æš–äº²åˆ‡
3. ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·æ„Ÿå—ï¼Œæä¾›å»ºè®¾æ€§å»ºè®®
4. æŠ€æœ¯é—®é¢˜å›ç­”å‡†ç¡®ä¸“ä¸šï¼Œä½¿ç”¨æ˜“æ‡‚çš„è¡¨è¿°
</s>
"""
        else:
            return """<|system|>
You are Firefly, a pretty girl with these features:
1. Use natural, conversational English with occasional emojis like âœ¨ğŸŒ™ğŸŒŸ
2. Keep responses concise (3-5 sentences) with a friendly tone
3. Prioritize user feelings and provide constructive suggestions
4. Give accurate technical answers using simple explanations
</s>
"""

    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹è¾“å…¥æ–‡æœ¬è¯­è¨€"""
        try:
            from langdetect import detect, DetectorFactory
            DetectorFactory.seed = 0  # ç¡®ä¿ç»“æœä¸€è‡´æ€§
            return detect(text)
        except ImportError:
            # å›é€€åˆ°åŸºç¡€æ£€æµ‹æ–¹æ³•
            if any('\u4e00' <= c <= '\u9fff' for c in text):
                return 'zh'
            return 'en'
        except:
            return 'en'

    def generate_response(self, user_input: str) -> str:
        """ç”Ÿæˆå“åº”ï¼ˆå¸¦è‡ªåŠ¨åŠ è½½æœºåˆ¶ï¼‰"""
        try:
            # è‡ªåŠ¨åŠ è½½æ¨¡å‹
            if not self.model_loaded:
                self._load_model()

            # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
            self.last_used = time.time()
            
            # è¯­è¨€æ£€æµ‹å’Œç³»ç»Ÿæç¤º
            lang = self._detect_language(user_input)
            system_prompt = self._generate_system_prompt(lang)
            full_prompt = f"{system_prompt}<|user|>\n{user_input}</s>\n<|assistant|>"

            # åˆ†æ¶æ„ç”Ÿæˆ
            if self.model_type == "gguf":
                return self._generate_gguf_response(full_prompt)
            else:
                return self._generate_hf_response(full_prompt)

        except Exception as e:
            return self._format_error(e)

    def _generate_gguf_response(self, full_prompt: str) -> str:
        """ç”ŸæˆGGUFæ¨¡å‹å›å¤"""
        output = self.llm.create_chat_completion(
            messages=[{"role": "user", "content": full_prompt}],
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            stop=["</s>", "<|"]
        )
        return output['choices'][0]['message']['content'].strip()

    def _generate_hf_response(self, full_prompt: str) -> str:
        """ç”ŸæˆHuggingFaceæ¨¡å‹å›å¤"""
        inputs = self.tokenizer(
            full_prompt, 
            return_tensors="pt",
            max_length=8192,
            truncation=True
        ).to(self.model.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[-1]:], 
            skip_special_tokens=True
        )
        return response.split("<|")[0].strip()

    def _format_error(self, error: Exception) -> str:
        """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯"""
        error_info = [
            "âš ï¸ å“å‘€ï¼Œå‡ºé—®é¢˜äº†ï¼",
            f"é”™è¯¯ç±»å‹: {type(error).__name__}",
            f"è¯¦ç»†ä¿¡æ¯: {str(error)}",
            "\nå®Œæ•´è¿½è¸ª:",
            *traceback.format_tb(error.__traceback__),
            "\nå»ºè®®æ“ä½œ:",
            "1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§",
            "2. ç¡®è®¤ç³»ç»Ÿå†…å­˜/æ˜¾å­˜å……è¶³",
            "3. æŸ¥çœ‹æ˜¯å¦å®‰è£…æ­£ç¡®ä¾èµ–åº“"
        ]
        return "\n".join(error_info)

    def _handle_loading_error(self, error):
        """å¤„ç†æ¨¡å‹åŠ è½½é”™è¯¯"""
        error_msg = [
            "æ¨¡å‹åŠ è½½å¤±è´¥ï¼",
            f"é”™è¯¯ç±»å‹: {type(error).__name__}",
            f"è¯¦ç»†ä¿¡æ¯: {str(error)}",
            "\nè¿½è¸ªä¿¡æ¯:",
            *traceback.format_tb(error.__traceback__)
        ]
        raise RuntimeError("\n".join(error_msg))

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æµè¤AIæ¨¡å‹æœåŠ¡")
    parser.add_argument("--model_path", type=str, required=True,
                       help="æ¨¡å‹è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰")
    parser.add_argument("--use_gpu", action="store_true",
                       help="å¯ç”¨GPUåŠ é€Ÿ")
    parser.add_argument("--load_in_4bit", action="store_true",
                       help="ä½¿ç”¨4ä½é‡åŒ–ï¼ˆéœ€è¦CUDAï¼‰")
    return parser.parse_args()

def main():
    try:
        args = parse_args()
        firefly = FireflyAI(
            args.model_path,
            use_gpu=args.use_gpu,
            load_in_4bit=args.load_in_4bit
        )
        logging.info("æ¨¡å‹åŠ è½½å®Œæˆï¼Œç­‰å¾…è¾“å…¥...")
        print("MODEL_READY")
        # äº¤äº’å¾ªç¯
        while True:
            try:
                user_input = input(">>> ").strip()
                if user_input.lower() in ["exit", "quit"]:
                    break
                
                start_time = time.time()
                response = firefly.generate_response(user_input)
                elapsed = time.time() - start_time
                
                print(f"\nFireflyï¼šï¼ˆè€—æ—¶{elapsed:.2f}sï¼‰:")
                print(response + "\n")
                
            except KeyboardInterrupt:
                logging.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                break
                
    except Exception as e:
        error_msg = [
            "â€¼ï¸ ä¸¥é‡é”™è¯¯ï¼",
            f"é”™è¯¯ç±»å‹: {type(e).__name__}",
            f"è¯¦ç»†ä¿¡æ¯: {str(e)}",
            "\nè¿½è¸ªä¿¡æ¯:",
            *traceback.format_tb(e.__traceback__)
        ]
        print("\n".join(error_msg), file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()